# Wprowadzenie do Uczenia Maszynowego – Notatki

## Podstawy uczenia maszynowego (ML)

Uczenie maszynowe (ang. *machine learning*) to podejście, w którym **komputery uczą się na podstawie danych i przykładów**, zamiast być jawnie zaprogramowane do wykonania zadania. Dzięki temu mogą rozwiązywać problemy intuicyjne dla ludzi, lecz trudne do formalnego opisania programistycznego, takie jak rozpoznawanie obrazów czy mowy. W uczeniu maszynowym komputer **wychwytuje wzorce z dostarczonych danych** i na ich podstawie samodzielnie buduje model podejmujący decyzje. Przykładowo, już we wczesnych zastosowaniach ML model regresji logistycznej wspomagał decyzje medyczne (np. o cesarskim cięciu), a naiwny klasyfikator Bayesowski skutecznie filtrował spam pocztowy – mimo braku bezpośredniego zaprogramowania tych reguł przez człowieka.

**Reprezentacja wiedzy (atrybuty)**: Skuteczność algorytmów ML zależy silnie od formatu danych wejściowych. Dane zazwyczaj reprezentuje się w postaci tzw. *atrybutów* (cech, ang. *features*), które opisują istotne właściwości przykładów. Klasyczne algorytmy nie modyfikują formy atrybutów – zakładają, że zostały one dobrze wybrane przez eksperta. W praktyce jednak dobór odpowiednich cech bywa trudny, zwłaszcza dla złożonych problemów (np. rozpoznawanie twarzy zależy od wielu czynników jak oświetlenie, kąt, zasłonięcie elementów twarzy itd.). **Uczenie reprezentacji** (*representation learning*) to idea, by algorytm sam uczył się zarówno zależności zadania, jak i optymalnej reprezentacji danych. Głębokie uczenie (ang. *deep learning*) jest przykładem – modele głębokie potrafią automatycznie tworzyć hierarchię cech: od niskopoziomowych (np. krawędzie na obrazie) do coraz bardziej złożonych (np. części obiektów, całe obiekty). Dzięki temu unikamy ręcznego projektowania cech i możemy efektywniej rozwiązywać skomplikowane zadania.

W uczeniu maszynowym wyróżnia się **uczenie z nadzorem** (dane treningowe zawierają oczekiwane odpowiedzi, model ma nauczyć się funkcji $f: X \to y$) oraz **uczenie bez nadzoru** (dane nie mają etykiet wynikowych, model ma np. odkryć wewnętrzną strukturę danych). Dla uczenia z nadzorem typowymi zadaniami są **klasyfikacja** (gdy atrybut decyzyjny $y$ jest kategoryczny) oraz **regresja** (gdy $y$ jest wartością liczbową). Przykładowo, klasyfikacja może dotyczyć rozpoznania, do której z kilku klas należy dany obiekt (np. czy obraz przedstawia kota czy psa), zaś regresja – przewidywania wartości ciągłej (np. ceny domu na podstawie cech). Innymi rodzajami problemów ML są m.in. transkrypcja (np. zamiana mowy na tekst), tłumaczenie maszynowe, wykrywanie anomalii, estymacja gęstości rozkładu, generowanie nowych przykładów (modelowanie generatywne) itp..

**Dane treningowe** najczęściej przyjmują postać tzw. *tabeli decyzyjnej*, w której wiersze odpowiadają przykładowym przypadkom (wektorom cech $x$), kolumny – atrybutom, a ostatnia kolumna (w uczeniu z nadzorem) zawiera etykiety decyzyjne $y$. Celem algorytmu jest uogólnić wiedzę z takich danych, aby poprawnie działał na nowych (niewidzianych) przykładach. W tym celu zwykle dzieli się dostępne dane na **zbiór treningowy** (do nauki modeli), **zbiór walidacyjny** (do strojenia hiperparametrów i porównywania modeli) oraz **zbiór testowy** (do ostatecznej oceny skuteczności na nieznanych danych).

## Perceptron – neuron i sieć jednowarstwowa

**Neuron biologiczny** stał się inspiracją dla sztucznych sieci neuronowych. Pojedynczy neuron biologiczny otrzymuje sygnały poprzez dendryty, przetwarza je w ciele komórki i wysyła wynik przez akson do innych neuronów. Kluczowe cechy neuronu to: nieliniowe przetwarzanie (wyjście nie jest prostą sumą wejść) oraz zdolność adaptacji – synapsy mogą wzmacniać lub osłabiać połączenia (uczenie się).

**Perceptron** to prosty matematyczny model neuronu sztucznego zaproponowany w latach 50. XX w. (Frank Rosenblatt, 1958). Składa się on z:

* $n$ **wejść** $x\_1,\dots,x\_n$ (odpowiednik dendrytów),
* $n$ **wag** $w\_1,\dots,w\_n$ przypisanych do tych wejść (odpowiednik siły synaps),
* **progu (biasu)** $\Theta$,
* pojedynczej **wyjściowej** wartości $y$.

Sygnał wyjściowy perceptronu obliczany jest jako: **najpierw suma ważona** netto $= \sum\_{i=1}^n w\_i x\_i$ (często nazywana $net$), a następnie zastosowanie **funkcji aktywacji**. W najprostszym wariancie progowym perceptronu *dyskretnego* wyjście $y$ przyjmuje wartości 0 lub 1 w zależności od tego, czy $net$ przekroczyło próg $\Theta$. Formalnie:

$$
y = 
\begin{cases} 
1 & \text{gdy } \sum_{i=1}^n w_i x_i \ge \Theta,\\ 
0 & \text{w przeciwnym razie.} 
\end{cases} 
$$

Tak zdefiniowany perceptron dokonuje **klasyfikacji binarnej** (dwuklasowej): decyduje czy przykład należy do klasy pozytywnej ($y=1$) czy negatywnej ($y=0$). Taki model nazywamy **dyskretnym perceptronem** (wyjście 0/1 albo -1/1). Istnieje też *perceptron ciągły*, którego wyjście $y$ jest wartością rzeczywistą (np. w zakresie \[0,1]) – może on służyć jako **regresor** przewidujący wartość liczbową. Historycznie rozwinięciem perceptronu ciągłego był model **ADALINE** (Widrow i Hoff, 1960), który uczył się przewidywać wartości liczbowe (regresję) za pomocą algorytmu stochastycznej metody gradientowej.

**Interpretacja geometryczna**: perceptron realizuje prosty klasyfikator liniowy. W przestrzeni cech $\mathbb{R}^n$ hiperpowierzchnia opisana równaniem $\mathbf{w}^T \mathbf{x} = \Theta$ (hiperpłaszczyzna) dzieli przestrzeń na dwa półprzestrzenie. Wektora wag $\mathbf{w} = (w\_1,\dots,w\_n)$ możemy interpretować jako wektor prostopadły do tej hiperplaszczyny decyzyjnej. Perceptron **aktywuje się (y=1)** dla wektorów wejściowych leżących po tej samej stronie hiperpowierzchni co $\mathbf{w}$, a daje $y=0$ dla wektorów po przeciwnej stronie. Umożliwia to rozróżnianie dwóch klas, **o ile są one liniowo separowalne** – tzn. można poprowadzić hiperplan rozdzielający poprawnie wszystkie przykłady obu klas.

**Ograniczenia perceptronu:** pojedynczy perceptron potrafi rozdzielać tylko zbiory danych, które są liniowo separowalne. Klasyczne przykłady: funkcje logiczne AND i OR są separowalne liniowo (perceptron może je modelować), lecz funkcja XOR – nie jest. To spostrzeżenie (Minsky i Papert, 1969) na długo zahamowało rozwój sieci neuronowych, gdyż sugerowało brak możliwości modelowania prostej funkcji XOR. Rozwiązaniem okazało się połączenie wielu neuronów w sieć wielowarstwową – co opisujemy poniżej.

**Sieć jednowarstwowa (warstwa wyjściowa perceptronów):** perceptron może zostać wykorzystany jako klasyfikator *dwuklasowy*. Aby rozszerzyć go na **wiele klas**, stosuje się sieć złożoną z *k kilku perceptronów* w warstwie wyjściowej. Każdy perceptron odpowiada za jedną klasę i uczy się rozpoznawać tę klasę (aktywizować się dla niej). Istnieją dwa podejścia do takiej architektury: **“lokalne” kodowanie wyjścia** – liczba neuronów wyjściowych = liczba klas, przy czym w poprawnej klasyfikacji dokładnie jeden neuron ma wyjście aktywne (sygnał 1) – wskazujący klasę. Alternatywne jest **“globalne” kodowanie** – liczba neuronów może być mniejsza niż klas, a decyzję odczytuje się z kombinacji ich wyjść (przy K klasach potrzeba co najmniej $\lceil \log\_2 K\rceil$ neuronów binarnych). W praktyce częściej stosuje się podejście lokalne (tzw. *one-hot*), bo jest prostsze w trenowaniu – sieć uczy się, by jeden neuron dawał najwyższą aktywację dla właściwej klasy, a pozostałe były nieaktywne. Warto zaznaczyć, że w wyjściowej warstwie sieci dla klasyfikacji wieloklasowej często używa się specjalnej funkcji aktywacji **softmax**, która przekształca surowe wyniki neuronów na postać rozkładu prawdopodobieństwa sumującego się do 1 (softmax przypisuje każdej klasie “wiarygodność”). Dla regresji (przewidywania wartości ciągłej) w warstwie wyjściowej stosuje się zazwyczaj **funkcję liniową**, aby model mógł generować dowolną wartość rzeczywistą.

**Uczenie perceptronu:** kluczowe osiągnięcie Rosenblatta to pokazanie, że perceptron potrafi **automatycznie nauczyć się odpowiednich wag** na podstawie danych treningowych. W przypadku perceptronu *dyskretnego* istnieje prosty algorytm iteracyjny zwany **regułą delta**. Polega on na tym, że przykłady uczące $(\mathbf{x}, d)$ (wektor cech oraz oczekiwana decyzja $d$) podaje się kolejno na wejście. Dla każdego przykładu porównuje się aktualną odpowiedź perceptronu $y$ z wartością oczekiwaną $d$. Jeśli perceptron popełnił błąd ($y \neq d$), następuje **modyfikacja wag** według wzoru:

$W' \leftarrow W + (d - y)\,\alpha\, X,$

gdzie $W$ to wektor wag, $X$ – wektor wejściowy, a $\alpha$ to mały współczynnik zwany **współczynnikiem uczenia** (ang. *learning rate*). Intuicyjnie: jeśli perceptron powinien był się aktywować ($d=1$) ale dał $y=0$, to $(d - y)=1$ i wektor wag zostanie przesunięty o mały krok w kierunku wektora $X$ (zwiększając iloczyn $W^T X$). Jeśli zaś perceptron błędnie się **nadaktywizował** ($d=0$, $y=1$), to $(d - y)=-1$ i nastąpi przesunięcie wag w przeciwnym kierunku – **oddalenie** od wektora $X$. Dzięki temu algorytm *wspina się* na prostej funkcji jakości w przestrzeni wag, zmierzając do rozwiązania. Dowodzi się, że jeśli dane są liniowo separowalne, taka reguła doprowadzi do znalezienia poprawnych wag w skończonej liczbie kroków. W praktyce **uczenie perceptronu wymaga wielokrotnego przejścia przez cały zbiór treningowy** zanim wagi ustabilizują się na dobrym rozwiązaniu.

*Uwaga:* Perceptron dyskretny nie jest różniczkowalny (funkcja progowa jest nieciągła), więc reguła delta to de facto prosty algorytm “wspinaczki” po przestrzeni wag, a nie klasyczna optymalizacja gradientowa. Dla perceptronu ciągłego (np. o wyjściu sigmoidalnym) można jednak zastosować metody gradientowe (o czym dalej). W praktyce dziś rzadko używa się oryginalnego algorytmu perceptronu – zastąpiły go uogólnione metody gradientowe, które działają także dla wielowarstwowych sieci neuronowych.

**Sieci wielowarstwowe:** ograniczenia pojedynczego perceptronu można przezwyciężyć przez łączenie wielu neuronów w sieć. **Sieć neuronowa** to układ neuronów połączonych warstwowo – wyjścia neuronów z warstwy $i$ są podłączone do wejść neuronów w warstwie $i+1$. **Jednowarstwowa sieć neuronowa** (ang. *single-layer neural network*) zazwyczaj oznacza tu sieć z *jedną warstwą ukrytą* (bo perceptrony wejściowe traktuje się jako warstwę 0). W praktyce termin ten bywa używany też dla samego perceptronu, więc by uniknąć nieporozumień lepiej mówić o sieci *jednopoziomowej* vs *wielopoziomowej*. **Wielowarstwowa sieć neuronowa (MLP – multilayer perceptron)** to po prostu kompozycja wielu funkcji realizowanych przez kolejne warstwy neuronów. Każda warstwa generuje pewną wewnętrzną reprezentację danych – można myśleć, że warstwa ukryta wylicza nowe, bardziej złożone *atrybuty* na podstawie poprzednich, a warstwa wyjściowa – na ich podstawie wynik końcowy. Dodanie warstw ukrytych pozwala sieci modelować **nieliniowe zależności** między wejściem a wyjściem – gdybyśmy nakładali tylko operacje liniowe, nawet wiele warstw zredukowałoby się do jednej warstwy liniowej. Dlatego w każdym neuronie stosuje się *nieliniową funkcję aktywacji*, co opisujemy poniżej.

Sieci wielowarstwowe – zwłaszcza głębokie (z wieloma warstwami ukrytymi) – mają ogromną **moc aproksymacyjną**. Zgodnie z *twierdzeniem o uniwersalnej aproksymacji* istnieje (przy pewnych założeniach technicznych) sieć neuronowa z **jedną warstwą ukrytą** i odpowiednio dużą liczbą neuronów nieliniowych, która jest w stanie aproksymować z zadaną dokładnością dowolną ciągłą funkcję od swoich wejść. Innymi słowy, już **dwuwarstwowa sieć (wejście -> ukryta -> wyjście)** jest teoretycznie zdolna do przedstawienia dowolnej zależności (o ile ma dość neuronów). Głębsze sieci nie są więc konieczne dla reprezentacji funkcji, ale w praktyce okazują się **bardziej efektywne** – wymagają znacznie mniej neuronów łącznie niż płytkie sieci do uzyskania podobnej złożoności reprezentowanej funkcji. Trudność długo polegała jednak na trenowaniu głębokich sieci – algorytmy były mało wydajne na ówczesnym sprzęcie i brakowało danych. Od około 2006 nastąpił przełom (Hinton, Bengio i in.) – pokazano metody skutecznej inicjalizacji i treningu głębokich architektur, co zapoczątkowało boom na **deep learning**【9†L137- L146】. Współcześnie głębokie sieci neuronowe osiągają rekordowe wyniki w wielu dziedzinach (wizja komputerowa, przetwarzanie języka, gry, itd.), często przewyższając inne techniki ML.

## Podstawowe funkcje aktywacji neuronu

Każdy neuron sumuje sygnały wejściowe ważone wagami, po czym wynik **$net$** przetwarza przez **funkcję aktywacji**, dając wartość wyjściową neuronu. **Funkcja aktywacji** wprowadza nieliniowość do działania neuronu – jest to kluczowe, bo złożenie dowolnej liczby funkcji liniowych dalej daje funkcję liniową (bez nieliniowości sieć nie zyska dodatkowej mocy ekspresyjnej). Funkcja aktywacji może mieć charakter **dyskretny** (np. próg 0/1) lub **ciągły** (daje wartość rzeczywistą) – w zależności od potrzeb. Neurony z **dyskretną** aktywacją są używane typowo do klasyfikacji (zwłaszcza w perceptronie pierwotnym), zaś **ciągłe** aktywacje są niezbędne w sieciach wielowarstwowych trenowanych metodami gradientowymi, a także przy regresji (ciągłe wyjście). Funkcje aktywacji można podzielić także na: **unipolarne** (zakres wartości od 0 do 1 dla ciągłej lub {0,1} dla dyskretnej) oraz **bipolarne** (od -1 do 1 dla ciągłej lub {-1,1} dla dyskretnej).

Najważniejsze typy funkcji aktywacji to:

* **Funkcja progowa (skokowa)** – klasyczny perceptron używa progu Heaviside’a: $y=1$ jeśli $net\ge 0$, w przeciwnym razie $y=0$. Jest to funkcja *nieliniowa*, ale **nieciągła** i niedziedziczna – nie posiada pochodnej w punkcie 0, więc nie nadaje się do metod gradientowych. Stosowana była w pierwszych perceptronach do problemów liniowo separowalnych. Wariant bipolarnej funkcji progowej zwraca -1 dla $net<0$ i +1 dla $net\ge 0$.

* **Funkcja znak (signum)** – właściwie podobna do progowej bipolarnej: $y = \operatorname{sign}(net)$ przyjmująca wartości -1 lub +1 w zależności od znaku argumentu. W literaturze bywa traktowana zamiennie z powyższą.

* **Funkcja liniowa** – $y = net$. Jest to funkcja ciągła i różniczkowalna (pochodna = 1), lecz *ściśle liniowa*. Jako aktywacja neuronów ukrytych jest bezużyteczna (skutkowałaby, że cała sieć jest złożeniem funkcji liniowych, a więc nadal liniowa). Znajduje zastosowanie w **warstwie wyjściowej dla zadań regresji**, gdzie potrzebujemy nieograniczonego zakresu liczb na wyjściu. Np. sieć aproksymująca funkcję $f(x)$ może mieć neurony wyjściowe liniowe, by móc przyjmować dowolne wartości rzeczywiste (wtedy minimalizacja błędu sprowadza się często do minimalizacji średniego błędu kwadratowego).

* **Funkcja sigmoidalna (logistyczna)** – ciągła, nieliniowa funkcja o charakterystycznym *S-kształtnym* wykresie (dla $x\to -\infty$ zbliża się asymptotycznie do 0, dla $x\to +\infty$ do 1). Ma postać:$\sigma(net) = \frac{1}{\,1 + e^{-net}\,},$ czyli zwraca wartość z zakresu $(0,1)$ (wariant *unipolarny*). Istnieje też wariant bipolarny: $\sigma\_b(net) = 2/(1+e^{-net}) - 1$, który daje zakres $(-1,1)$. Sigmoidalna aktywacja była przez lata podstawowym wyborem dla neuronów ukrytych w sieciach – **jest gładka (ciągła i różniczkowalna wszędzie)** oraz **monotoniczna rosnąca**, co upraszcza analizę. Jej ważną cechą jest tzw. *własność wzmacniania*: ma dodatnią krzywiznę dla ujemnych argumentów i ujemną dla dodatnich, co powoduje, że dla wartości bliskich ekstremów (0 lub 1) ulega *nasyceniu* – zmiany wejścia prawie nie wpływają na wyjście (pochodna bliska zera), natomiast dla wartości średnich jest najbardziej czuła. Co istotne, **pochodna funkcji sigmoidalnej można wyrazić przez samą funkcję**: $\sigma'(net) = \sigma(net),(1-\sigma(net))$, dzięki czemu łatwo ją wykorzystać w algorytmie wstecznej propagacji błędu. Sigmoidy dobrze sprawdzają się, gdy chcemy *wymusić wyjście w określonym przedziale* (np. prawdopodobieństwo klasy 0-1). Przykładowo w sieci binarnej klasyfikacji neuron wyjściowy z aktywacją sigmoidalną może modelować $P(y=1|x)$ – prawdopodobieństwo przypisania obiektu do klasy 1. W istocie w modelu regresji logistycznej używa się właśnie pojedynczego neuronu sigmoidalnego, trenowanego metodą maksymalizacji wiarygodności (co odpowiada minimalizacji tzw. entropii krzyżowej – zob. dalej).

&#x20;*Wykres funkcji sigmoidalnej (logistycznej). Wartości wyjściowe asymptotycznie zbliżają się do 0 dla dużych ujemnych $net$ oraz do 1 dla dużych dodatnich $net$. Funkcja jest gładka i monotoniczna, dzięki czemu nadaje się do sieci trenowanych gradientowo.*

* **Funkcja $\tanh$ (hyperboliczna tangens)** – również sigmoidalna S-kształtna funkcja, jednak *bipolarna* (zakres od -1 do 1). Opisana jest wzorem $\tanh(net) = \frac{e^{net}-e^{-net}}{,e^{net}+e^{-net},}$. Często używana jako alternatywa sigmoidy unipolarnej – jej wartości są zbalansowane wokół zera, co bywa korzystne (zapobiega przesunięciu sygnałów w stronę dodatnią). Pochodna: $\tanh'(net) = 1 - \tanh^2(net)$. Podobnie jak logistyczna – ma problem nasycania (pochodna maleje do zera na krańcach). Empirycznie, w klasycznych sieciach często dawała nieco lepsze rezultaty niż sigmoida unipolarna, ale obecnie i tak ustąpiła pola nowszym aktywacjom.

* **Funkcja ReLU** (ang. *Rectified Linear Unit*) – najpopularniejsza obecnie funkcja aktywacji, szczególnie w warstwach ukrytych sieci głębokich. Jej definicja to: $\operatorname{ReLU}(net) = \max(0,;net)$. Innymi słowy: dla argumentu ujemnego ReLU daje 0, a dla nieujemnego – zwraca identyczną wartość (przepuszcza ją liniowo). Wykres ReLU to linia pozioma dla $x<0$ i ukośna ($y=x$) dla $x\ge 0$.

&#x20;*Wykres funkcji ReLU – wartości ujemne są obcinane do zera (neurony nieaktywne), a dla dodatnich wejść funkcja rośnie liniowo (pochodna = 1). Dzięki temu ReLU łączy nieliniowość z zachowaniem cech funkcji liniowej.*

Funkcja ReLU ma kilka zalet, które przyczyniły się do jej popularności. Jest **bardzo prosta i szybka obliczeniowo**, a mimo nieliniowości zachowuje częściowo charakter liniowy (dla $x>0$), co sprzyja uogólnianiu modelu. Ważne jest też, że ReLU **nie nasyca się dla dużych argumentów dodatnich** – jej pochodna wynosi 1 w całym zakresie $x\ge 0$. To oznacza brak zanikania gradientu w trakcie uczenia dla aktywnych neuronów, co było bolączką sieci sigmoidalnych (gdzie neurony wchodziły w nasycenie i przestawały się uczyć). Empiryczne badania wykazały, że w głębszych sieciach **ReLU pozwala trenować skuteczniej i osiągać lepsze wyniki** niż klasyczne funkcje sigmoidalne czy $\tanh$ (m.in. dzięki redukcji problemu zanikającego gradientu). ReLU ma oczywiście swoje wady (np. dla $x<0$ gradient jest zerowy – neurony mogą “umierać” przy złej inicjalizacji), niemniej istnieją jej warianty rozwiązujące te problemy: *leaky ReLU* (dopuszcza małe ujemne wyjścia), *parametric ReLU*, *ELU*, *softplus* itp.. W praktyce jednak zwykła ReLU wciąż sprawdza się najlepiej w wielu zadaniach i jest domyślnym wyborem dla warstw ukrytych.

**Inne funkcje aktywacji**: Istnieje wiele innych funkcji, np. **funkcje radialne (RBF)**, używane czasem w warstwach ukrytych (np. w sieciach RBF), różne funkcje nasycające (inna odmiana *soft* i *hard tanh*, *softsign* itp.), funkcja **softmax** (w warstwie wyjściowej do klasyfikacji wieloklasowej, o czym wspomniano) oraz egzotyczne propozycje badawcze. W praktyce jednak, dopóki nie udowodni się, że nowa funkcja bije na głowę standardowe – **stosuje się sprawdzone rozwiązania**. Obecnie większość sieci głębokich używa w ukrytych warstwach ReLU (lub pokrewnych), a w wyjściowej – sigmoidy (dla zadań binarnych) lub softmax (dla wieloklasowych).

## Uczenie sieci neuronowej – funkcja straty

Nauczenie sieci (lub innego modelu) oznacza **znalezienie takich wartości parametrów (wag i progów neuronu), by model dobrze realizował zadane zadanie**. W przypadku uczenia z nadzorem formalizuje się to jako problem **optymalizacji**: definiujemy funkcję *straty* (błędu) mierzącą, jak bardzo przewidywania modelu odbiegają od oczekiwanych wyników, i staramy się tę funkcję zminimalizować poprzez zmianę parametrów.

Najczęściej zakłada się, że mamy zdefiniowaną funkcję straty $L(f(x;\Theta),,y)$ dla pojedynczego przykładu $(x,y)$. Przykładowo może to być błąd kwadratowy: $L(y^{pred},y^{true}) = \frac{1}{2}(y^{pred}-y^{true})^2$ albo *log-loss* dla klasyfikacji binarnej: $L(y^{pred},y^{true}) = -\big(y \ln y^{pred} + (1-y)\ln(1-y^{pred})\big)$, itp. Na zbiorze danych definiujemy **funkcję kosztu (błędu) całego modelu** jako średnią (lub sumę) strat ze wszystkich przykładów:

$J(\Theta) \;=\; \frac{1}{m}\sum_{i=1}^m L(f(x^{(i)};\Theta),\; y^{(i)}),$

gdzie $m$ to liczba przykładów treningowych. Funkcja $J(\Theta)$ zależy od wektora wszystkich parametrów modelu $\Theta$. Celem uczenia jest znalezienie minimum tej funkcji. Gdybyśmy znali prawdziwy rozkład danych, minimalizacja średniej straty byłaby znalezieniem *ryzyka minimalnego* (tzw. **ryzyko** to wartość oczekiwana straty na losowym przykładzie). W praktyce jednak rozkład jest nieznany – dysponujemy skończoną próbką danych. Minimalizujemy więc **ryzyko empiryczne** (empiryczną średnią strat na zbiorze treningowym). Trzeba uważać, by model nie zaczął “uczyć się na pamięć” danych treningowych kosztem generalizacji – jest to zjawisko **przetrenowania (overfitting)**. Proste minimalizowanie błędu na zbiorze uczącym może do tego prowadzić, dlatego stosuje się metody regularyzacji opisane dalej, a także monitoruje błąd na zbiorze walidacyjnym.

**Rodzaje funkcji straty:** dobór funkcji straty zależy od rodzaju problemu. Dla **regresji** naturalną miarą jest błąd kwadratowy lub absolutny. Dla **klasyfikacji** – często stosuje się *stratę krzyżowej entropii* (związaną z logarytmem prawdopodobieństwa). Bezpośrednia strata 0-1 (licząca błędne klasyfikacje) nie nadaje się do optymalizacji gradientowej, bo jest dyskretna i nigdzie niezróżniczkowalna. Zamiast niej minimalizuje się funkcję **zastępczą (surrogate)** – np. właśnie entropię krzyżową albo tzw. *hinge loss* – które są gładkie i łatwiejsze do optymalizacji. Ważne, że minimalizacja straty zastępczej często prowadzi też do minimalizacji błędu 0-1 na zbiorze testowym, nawet jeśli na zbiorze uczącym osiągnęliśmy już zero błędów.

Konkretnie, w nowoczesnych sieciach neuronowych na ogół stosuje się podejście **MLE (maksimum likelihood)** – czyli wybiera się parametry maksymalizujące prawdopodobieństwo danych treningowych. Okazuje się, że dla typowych modeli prowadzi to właśnie do funkcji kosztu postaci entropii krzyżowej. Przykładowo, jeśli sieć o parametrach $\Theta$ zwraca dla każdego $x$ rozkład prawdopodobieństwa $p\_{\Theta}(y|x)$ (np. poprzez softmax na wyjściu), to funkcja kosztu wynikająca z MLE ma postać:

$J(\Theta) = -\,\mathbb{E}_{(x,y)\sim \hat{p}_{data}}\; \ln\, p_{\Theta}(y|x),$

czyli minus logarytm wiarygodności (co sprowadza się do minimalizacji entropii krzyżowej między rozkładem modelu a rozkładem danych). Dla klasyfikacji binarnej jest to dokładnie log-loss jak wyżej. Natomiast w przypadku regresji, jeśli założymy rozkład normalny błędu, to MLE prowadzi do minimalizacji średniego błędu kwadratowego (MSE). Zaletą podejścia przez MLE jest to, że **nie musimy ręcznie wymyślać funkcji straty dla każdego problemu** – wynika ona z założeń probabilistycznych modelu.

Warto pamiętać, że funkcje aktywacji neuronów wyjściowych często dobiera się **wspólnie z funkcją straty**. Przykład: jeśli chcemy przewidywać prawdopodobieństwo wystąpienia zdarzenia (klasy 1), dobierzemy wyjściową aktywację sigmoidalną $\sigma$, a funkcję kosztu – logarytmiczną (cross-entropy). Jeśli chcemy estymować wartość rzeczywistą (regresja) – użyjemy wyjścia liniowego i np. MSE. Dla klasyfikacji wieloklasowej – wyjście softmax i stratę krzyżowej entropii dla rozkładu wielomianowego (tzw. *multinoulli*).

## Pochodne, gradient i metoda gradientowa

Aby zminimalizować funkcję kosztu $J(\Theta)$, kluczowe znaczenie ma jej **pochodna** względem optymalizowanych parametrów. W przypadku funkcji jednej zmiennej, pochodna $J'(\theta)$ wskazuje kierunek zwiększania się lub zmniejszania funkcji. **Gradient** jest uogólnieniem pochodnej na funkcje wielu zmiennych: $\nabla\_{\Theta} J$ to wektor pochodnych cząstkowych $\bigl[\partial J/\partial \theta_1,\dots,\partial J/\partial \theta_n\bigr]$. Kierunek gradientu to kierunek najsilniejszego wzrostu wartości funkcji – w przestrzeni wielowymiarowej zachodzi formalnie, że spośród wszystkich jednostkowych wektorów $u$ największa pochodna kierunkowa jest równa $| \nabla J|$ i występuje dla wektora $u$ równoległego do $\nabla J$. Dlatego, aby **zmniejszyć wartość $J$**, należy poruszać się w przeciwnym kierunku do gradientu – tam funkcja opada najszybciej (największy spadek kierunkowy). Punkty, w których gradient $\nabla J = 0$, to **punkty krytyczne** – mogą to być minima, maksima lub punkty siodłowe (gdzie nachylenie jest zerowe, ale nie jest to minimum lokalne).

**Metoda najszybszego spadku (gradientowa)**: jest to iteracyjny algorytm optymalizacji, w którym startujemy od pewnego losowego rozwiązania $\Theta^{(0)}$ i następnie przesuwamy się małymi krokami przeciwnie do gradientu, aż dotrzemy do minimum funkcji (lokalnego lub globalnego). W uproszczeniu:

$\Theta \leftarrow \Theta - \eta\, \nabla_{\Theta} J(\Theta),$

gdzie $\eta$ (eta) to niewielki **krok (współczynnik uczenia)** określający, jak duża zmiana parametrów jest wykonywana w jednej iteracji. Powyższa formuła to **gradient descent** (spadek wzdłuż gradientu). Jeśli $J(\Theta)$ jest różniczkowalna i ma pojedyncze minimum globalne, taka iteracja (przy odpowiednio małym $\eta$) zbiegnie w okolice minimum. W praktyce jednak funkcje kosztu głębokich modeli są bardzo złożone (wiele minimów lokalnych, punktów siodłowych, płaskich obszarów), więc wybór dobrej metody optymalizacji jest ważny.

**Stochastyczna metoda gradientowa (SGD)**: Zauważmy, że do obliczenia dokładnego gradientu $\nabla J(\Theta)$ potrzebujemy zsumować pochodne błędów dla **wszystkich** $m$ przykładów (jak wyżej). Dla dużych zbiorów danych jest to kosztowne obliczeniowo. W praktyce stosuje się więc **oszacowanie gradientu na podstawie małej losowej podpróby** danych – stąd nazwa metoda stochastyczna. Wybiera się np. losowo kilkanaście lub kilkadziesiąt przykładów (tzw. **minibatch**) i na ich podstawie liczy gradient przybliżony:

$g \;\approx\; \frac{1}{m_0}\sum_{i=1}^{m_0} \nabla_{\Theta} L(x^{(i)}, y^{(i)}, \Theta),$

gdzie $m\_0 \ll m$. Następnie wykonuje się aktualizację $\Theta \leftarrow \Theta - \eta, g$. Losowe dobieranie porcji danych powoduje, że kolejne kroki są nieco “hałaśliwe” – funkcja kosztu nie zawsze maleje jednostajnie, ale za to w skali całego procesu poruszamy się we właściwym kierunku znacznie szybciej, wykonując dużo tańszych przybliżonych aktualizacji zanim przejdziemy przez cały zbiór danych. **SGD** jest obecnie standardem w uczeniu głębokich sieci – pozwala efektywnie trenować modele na milionach przykładów. Warianty tego algorytmu wprowadzają dodatkowe usprawnienia: np. **momentum** (dodaje do kroku “pęd” z poprzednich iteracji, by wygładzić trajektorię), **adaptacyjne kroki** (jak w algorytmach AdaGrad, RMSProp, Adam – samoczynnie dostosowują współczynnik uczenia dla każdych wag) czy **malejący współczynnik uczenia** (zmniejszanie $\eta$ w trakcie trenowania, by zapewnić zbieżność do minimum).

Należy pamiętać, że funkcja kosztu sieci może mieć wiele minimów lokalnych o podobnej wartości (a nawet *punkty siodłowe* – rozwiązania niestabilne). SGD zazwyczaj radzi sobie z tym, ponieważ hałas pozwala mu “wyskoczyć” z płytkich lokalnych minimów. Niemniej jakość rozwiązania może zależeć od inicjalizacji wag i doboru hiperparametrów. Używa się wielu praktyk (inicjalizacja Xavier/He, odpowiednie normalizacje, itp.), aby proces trenowania był stabilny.

## Wsteczna propagacja błędu (algorytm Backpropagation)

Wielowarstwowe sieci neuronowe zawierają pośrednie warstwy ukryte, których parametry także trzeba dostosować podczas uczenia. Jak obliczyć, jak bardzo dana waga w ukrytej warstwie wpływa na końcowy błąd? Rozwiązaniem jest algorytm **wstecznej propagacji błędu** (ang. *back-propagation*, w skrócie **BP**).

Uczenie sieci odbywa się iteracyjnie w następujących krokach: najpierw wykonujemy **przejście w przód** (ang. *forward pass*) – obliczamy wyjścia neuronów kolejnych warstw aż do uzyskania wyniku końcowego i wartości funkcji straty $J(\Theta)$ na wyjściu. Następnie algorytm wykonuje **przechodzenie wstecz**: propaguje informację o błędzie od wyjścia w kierunku wejść, warstwa po warstwie, obliczając po drodze **gradienty** funkcji kosztu względem wag każdej warstwy. Dzięki zastosowaniu **reguły łańcuchowej** różniczkowania (znanej z analizy matematycznej) możemy efektywnie wyznaczyć pochodne cząstkowe błędu względem wszystkich parametrów, **wykorzystując pośrednie wyniki z etapu forward**. Intuicyjnie, backprop określa, jak zmiana stanu danego neuronu o jednostkę wpłynęłaby na końcowy błąd, i tę informację przenosi dalej w głąb sieci. Końcowym efektem są gradienty $\frac{\partial J}{\partial w\_{ij}^{(l)}}$ dla każdej wagi $w\_{ij}^{(l)}$ (łączącej neuron $j$ w warstwie $(l-1)$ z neuronem $i$ w warstwie $l$). Mając gradienty, algorytm aktualizuje wagi (np. metodą SGD opisaną wyżej) i przechodzi do kolejnej iteracji z nowymi parametrami.

Warto podkreślić dwie rzeczy:
**(1)** Backpropagation to *algorytm obliczania gradientu* – wykorzystuje sprytnie faktoryzację obliczeń, by uniknąć wielokrotnego liczenia tych samych pochodnych. Mnożąc **jakobiany** kolejnych przekształceń (warstw) sieci, uzyskujemy gradient funkcji złożonej. Koszt obliczeń rośnie tylko liniowo z liczbą parametrów sieci (dokładniej: z liczbą połączeń neuronów), co czyni ten algorytm bardzo wydajnym – to w istocie zastosowanie metody *reverse mode differentiation* dla funkcji wyrażonej jako graf obliczeń. **(2)** Backprop **nie jest sam w sobie algorytmem uczenia** – to tylko metoda wyznaczania gradientów. Sam proces uczenia to np. iteracyjne aktualizacje wag metodą gradientową, w którym to backprop dostarcza informacji jak te wagi korygować. Dlatego często precyzuje się, że używamy np. “SGD z propagacją wsteczną” jako pełnej nazwy procedury trenowania sieci. Co ważne, backprop jest algorytmem ogólnym – można go zastosować do **dowolnego modelu obliczeniowego przedstawionego w postaci grafu zależności matematycznych** (nie tylko do sieci neuronowych).

Algorytm wstecznej propagacji błędu został sformułowany w latach 80. (Rumelhart, Hinton i Williams, 1986) i był kamieniem milowym – umożliwił efektywne uczenie wag w sieciach z *jedną* warstwą ukrytą, a z czasem (po udoskonaleniach) również w głębszych sieciach. W praktyce obecnie korzysta się z implementacji BP dostępnych w bibliotekach – warto jednak rozumieć, że działa on w oparciu o standardową regułę łańcuchową różniczkowania. W pseudokodzie: zapisujemy wyrażenia obliczane przez sieć (tzw. *graf obliczeń*), potem obliczamy wartości wszystkich węzłów grafu od wejścia do wyjścia (to forward pass). Następnie oznaczamy gradient węzła wyjściowego (np. $dJ/dJ = 1$) i **rekurencyjnie** liczymy gradienty dla poprzedników zgodnie z pochodnymi lokalnych operacji. Dzięki przechowywaniu wyników cząstkowych z forward pass, unikamy ponownego liczenia wielu wyrażeń – każda krawędź grafu jest *odwiedzana raz* w trakcie propagacji wstecz. W ten sposób koszt obliczenia wszystkich gradientów jest porównywalny z kosztem dwukrotnego wykonania sieci (w przód i w tył).

## Sieci konwolucyjne (CNN)

**Sieci konwolucyjne** (ang. *Convolutional Neural Networks*, **CNN**) to specjalna architektura sieci neuronowej zaprojektowana głównie do przetwarzania danych o strukturze przestrzennej (np. obrazów). Inspiracją biologiczną dla CNN była warstwowa budowa kory wzrokowej w mózgu – neurony wizyjne reagują na proste wzorce w małych polach recepcyjnych, a wyższe warstwy reagują na kombinacje tych wzorców (np. krawędzie, potem kształty itd.). Już w latach 80. stworzono pierwsze sieci konwolucyjne (np. Neocognitron, 1980), jednak prawdziwy rozkwit nastąpił po 2012 roku, kiedy to głęboka CNN (AlexNet) zdeklasowała inne metody w konkursie ImageNet na rozpoznawanie obrazów, obniżając błąd klasyfikacji z 26% do 15%. Od tego czasu CNN stały się dominującym podejściem w zadaniach takich jak rozpoznawanie obiektów na zdjęciach, detekcja i lokalizacja obiektów, analiza wideo, a nawet zadania spoza wizji – np. rozpoznawanie mowy czy przetwarzanie języka (gdzie dane da się ułożyć w strukturę “obrazopodobną”).

**Idea działania CNN:** zamiast łączyć **każdy neuron z każdą cechą wejścia** (jak w pełni połączonych MLP), sieć konwolucyjna wykorzystuje **lokalne połączenia** i **współdzielenie wag**. Neurony ułożone są w **mapy cech** – każda mapa to zbiór neuronów, które **działają na niewielki podobszar wejścia** (np. mały fragment obrazu) i **dzielą między sobą zestaw wag** (tzw. filtr konwolucyjny). Taki filtr przesuwany jest krok po kroku po obrazie (stąd “splot” – konwolucja), wykrywając w całym polu widzenia ten sam rodzaj wzorca. W efekcie CNN potrafi wychwytywać cechy niezależnie od ich położenia w obrazie – ma wbudowaną **niezmienniczość na translacje (przesunięcia)** dzięki współdzieleniu wag. Ponadto ograniczenie połączeń do lokalnych regionów wprowadza założenie **lokalnej spójności cech** – czyli że istotny wzorzec da się rozpoznać patrząc na niewielki fragment danych (np. krawędź rozpoznajemy lokalnie). To założenie sprawdza się w obrazach, dźwięku itp. i **dramatycznie zmniejsza liczbę parametrów** w porównaniu do sieci w pełni połączonych. Mniej parametrów to mniejsze ryzyko przeuczenia i mniejsze zapotrzebowanie na dane.

Typowa **architektura CNN** składa się z **naprzemiennych warstw splotowych i warstw *zbierających* (ang. pooling)**. **Warstwa konwolucyjna** wykonuje opisane powyżej operacje wielu filtrów przesuwających się po wejściu, produkując odpowiadające im mapy cech (każdy filtr = jedna mapa cech wyjściowych). **Warstwa **zbierająca**** (*pooling*) służy do zmniejszenia rozmiaru reprezentacji i wprowadza dodatkową niezmienniczość. Typowy pooling to np. **maksimum z okna** $2\times 2$ piksele – zastępuje cztery liczby ich maksimum. To zmniejsza rozdzielczość map cech o połowę w każdym wymiarze, redukując dane i skupiając się na najsilniejszych sygnałach. Po kilku takich naprzemiennych operacjach otrzymujemy **wysokopoziomowe cechy** o małej rozdzielczości (np. po przejściu całego obrazu 256×256 przez 5 bloków conv+pool może zostać np. 8 map 8×8 reprezentujących złożone wzory). Na końcu CNN zwykle *“spłaszcza”* te mapy cech do wektora i dołącza klasyczne **warstwy w pełni połączone** (jak MLP) prowadzące do wyjścia sieci – np. klasyfikacji do k kategorii poprzez softmax.

Sieci konwolucyjne zdobyły popularność dzięki temu, że **uwzględniają strukturę danych** (np. obrazu) bez potrzeby ręcznego inżynierii cech. Uczą się hierarchii cech: pierwsze warstwy wykrywają proste kształty (krawędzie, tekstury), kolejne składają z nich bardziej złożone obiekty (np. części twarzy), a ostatnie – całościowe koncepcje (np. rozpoznanie gatunku zwierzęcia na zdjęciu). Cechuje je: **lokalność połączeń**, **współdzielenie wag** i **hierarchiczność reprezentacji**. Choć powstały z myślą o obrazach, adaptuje się je też do innych danych – np. do sekwencji czasowych (1D convolution) lub do danych tekstowych (gdzie “obrazem” jest macierz zakodowanych słów w zdaniu). CNN stały się fundamentem nowoczesnej **wizji komputerowej**.

## Sieci rekurencyjne (RNN) – sekwencje i pamięć

**Sieci rekurencyjne** (ang. *Recurrent Neural Networks*, **RNN**) to rodzina sieci neuronowych przystosowana do przetwarzania **danych sekwencyjnych** (ciągów o zmiennej długości): tekstu, dźwięku, sekwencji zdarzeń, itp. Cechą charakterystyczną RNN jest istnienie **połączeń zwrotnych** – neurony mogą przekazywać sygnał *z powrotem* do neuronów poprzednich (w praktyce oznacza to sprzężenia, gdzie stan neuronu z poprzedniego kroku czasowego wpływa na jego stan w kolejnym). W sieci rekurencyjnej nie ma z góry ustalonego kierunku przepływu informacji tylko od wejścia do wyjścia – sygnały mogą krążyć, tworząc wewnętrzną **pętlę pamięci**. Dzięki temu RNN potrafią **“zapamiętywać” poprzednie elementy sekwencji** i uwzględniać kontekst wcześniejszych danych przy przetwarzaniu kolejnych. Przykładowo, sieć czytająca zdanie może zachować w swoich stanach wewnętrznych informację o poprzednich słowach.

Najprostszy model RNN można wyobrazić sobie jako zwykły neuron (np. z aktywacją $\tanh$) otrzymujący na wejściu nie tylko aktualny sygnał $x\_t$, ale też **sprzężenie zwrotne** ze swojego wyjścia z poprzedniego kroku czasowego $y\_{t-1}$. Taki neuron rekurencyjny może być “rozwinięty w czasie” – widząc ciąg $x\_1, x\_2, ..., x\_T$ generuje kolejne wyjścia $y\_1,\dots,y\_T$, a każdy z nich zależy od bieżącego $x\_t$ **oraz** od stanu neuronu po przetworzeniu poprzednich $t-1$ elementów. W ten sposób sieć rekurencyjna może operować na sekwencjach **dowolnej długości** – w przeciwieństwie do sieci jednokierunkowej (feed-forward), która ma z góry określoną liczbę wejść/wyjść.

**Zastosowania RNN:** wszędzie tam, gdzie dane mają charakter sekwencji lub czasowej zależności. Przykłady: **modelowanie języka naturalnego (NLP)** – sieci rekurencyjne potrafią generować tekst słowo po słowie, tłumaczyć zdania (sekwencja słów wejściowych -> sekwencja wyjściowych) itd. Są używane w systemach tłumaczeń (tzw. architektury seq2seq), do analizy sentymentu wypowiedzi, rozpoznawania mowy (gdzie sygnał audio jest sekwencją próbek), a nawet generowania muzyki czy opisów obrazów. W robotyce RNN mogą np. uczyć się sterowania – gdzie sygnały z czujników tworzą strumień danych w czasie. Istnieją także **rekurencyjne sieci o specjalnej architekturze** do nienadzorowanego uczenia reprezentacji (np. **sieci Kohonena** – mapy samoorganizujące). Ogólnie RNN są bardzo potężne, ale też **trudniejsze w analizie i trenowaniu** niż zwykłe sieci (co wynika z dodatkowych sprzężeń zwrotnych).

**Problemy z RNN:** podstawowy mechanizm przekazywania stanu posiada wadę – tzw. **problem zanikania i eksplozji gradientu**. Gdy próbujemy uczyć RNN metodą gradientową (np. analogiczną do backprop, ale w czasie – tzw. BPTT, *backpropagation through time*), pochodne błędu względem odległych w czasie parametrów stają się ekstremalnie małe lub duże (efekt mnożenia wielu macierzy wag w kolejnych krokach). W praktyce oznacza to, że **RNN “zapominają” długoterminowe informacje** – wpływ dawnych wejść na obecne wyjście zanika wykładniczo z każdym krokiem, więc po kilkudziesięciu krokach niemal nie pamiętają początków sekwencji. To bywa niepożądane – np. przy analizie długiego zdania pierwsze słowa mogą być kluczowe dla sensu ostatniego słowa, a zwykła RNN może o nich “zapomnieć”. Innym ujęciem jest to, że standardowa RNN ma małą *pojemność pamięci*.

**LSTM – długotrwała pamięć**: przełomem w trenowaniu RNN okazała się architektura **Long Short-Term Memory** (**LSTM**) zaproponowana przez Hochreitera i Schmidhubera (1997). LSTM to specjalny *neuronalny moduł (komórka) z pamięcią*, który dodaje mechanizmy pozwalające przechowywać informacje przez długi kontekst czasowy. LSTM wprowadza dwa rodzaje stanów wewnętrznych: **stan komórki $c\_t$ (pamięć długoterminowa)** oraz **stan ukryty $h\_t$ (krótkoterminowy)**. W każdej iteracji (kroku czasowym) LSTM **decyduje trójkami tzw. bramek** co zrobić z tymi stanami:

* **bramka zapominania** $f\_t$ – kontroluje, które informacje w stanie $c\_{t-1}$ usunąć (zdyskontować),
* **bramka wejścia** $i\_t$ – decyduje, które nowe informacje (kandydat $g\_t$ wyliczony podobnie jak neuron standardowy) zapisać do stanu komórki,
* **bramka wyjścia** $o\_t$ – decyduje, jaka część zaktualizowanego stanu $c\_t$ pokazać na wyjściu (które jest jednocześnie stanem $h\_t$).

Warto zauważyć, że bramki $f, i, o$ są realizowane poprzez *sigmoidalne neurony* – przyjmują wartości w $(0,1)$, co mnożone przez wektor stanu działa jak zawór: 1 oznacza pełne przepuszczenie informacji, 0 – pełne zablokowanie. Dzięki temu LSTM może **podtrzymywać pewne informacje przez wiele kroków** (gdy $f\_t \approx 1$ i $i\_t \approx 0$, komórka utrzymuje swój stan) lub szybko zapominać nieistotne rzeczy ($f\_t \approx 0$). Uczy się tych operacji gradientowo, tak by minimalizować błąd predykcji sekwencji.

Matematycznie działanie pojedynczej komórki LSTM można opisać wzorami (dla uproszczenia pomijamy biasy i przyjmujemy, że wszystkie bramki oraz kandydat $g\_t$ są odrębnymi neuronami z odpowiednimi wagami):

$$
\begin{aligned}
i_t &= \sigma(W_{xi} x_t + W_{hi} h_{t-1}),\\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1}),\\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1}),\\
g_t &= \tanh(W_{xg} x_t + W_{hg} h_{t-1}),\\
c_t &= f_t \odot c_{t-1} \;+\; i_t \odot g_t,\\
h_t &= o_t \odot \tanh(c_t),
\end{aligned}
$$

gdzie $\sigma$ to funkcja sigmoidalna (0-1), a $\odot$ oznacza mnożenie po elementach wektora. W powyższych równaniach $W_{x\ast}$ i $W_{h\ast}$ to macierze wag wejścia i stanu poprzedniego dla odpowiednich bramek. Jak widać, poprzedni stan $c\_{t-1}$ jest przenoszony do $c\_t$ po przemnożeniu przez $f\_t$ (czyli część jest zachowana), a nowy kandydat $g\_t$ (przetworzony aktualny input) dodaje się skalowany bramką $i\_t$. Wyjście $h\_t$ to po prostu “obserwowalna” część stanu wewnętrznego, przefiltrowana bramką wyjścia.

W praktyce LSTM okazało się **znacznie efektywniejsze** od standardowych RNN – może przechowywać zależności długoterminowe, szybciej się uczy i ma większą pojemność (zdolność do modelowania skomplikowanych relacji). Stało się podstawowym blokiem dla wielu aplikacji sekwencyjnych. Istnieją również uproszczone warianty, np. **GRU (Gated Recurrent Unit)** – łączące bramkę zapominania i wejścia w jedną, co redukuje liczbę parametrów. Współcześnie standardem jest, że **dla danych sekwencyjnych używa się sieci rekurencyjnej LSTM lub GRU** jako bazowego modelu. Mimo udoskonaleń, trenowanie bardzo głębokich RNN (wiele warstw rekurencyjnych) wciąż bywa trudne i podatne na przeuczenie – często stosuje się kombinacje z technikami regularyzacji (np. *dropout* między warstwami rekurencyjnymi).

## Metody regularyzacji sieci

**Regularyzacja** to wszelkie techniki zapobiegające **przetrenowaniu (overfittingowi)** modelu na danych treningowych, czyli poprawiające jego uogólnienie na dane testowe. W kontekście sieci neuronowych stosuje się kilka podstawowych metod:

* **Regularyzacja karą na wagach (normy $L^2$ i $L^1$)** – do funkcji kosztu dodaje się dodatkowy składnik $\Omega(\Theta)$ rosnący wraz z “wielkością” wag, co wymusza ich ograniczenie. Najpopularniejsza jest **regularyzacja $L^2$** znana jako ***weight decay*** (zanikanie wag): $\Omega(\Theta) = \frac{\alpha}{2}|\mathbf{w}|\_2^2 = \frac{\alpha}{2}\sum\_i w\_i^2$. Dodanie tej kary powoduje, że gradient błędu ma dodatkowy składnik $\alpha w\_i$ dla każdej wagi, a zatem w każdej iteracji uczenia **wagi są skalowane (pomniejszane) o stały ułamek**. W efekcie rozwiązanie preferuje mniejsze wartości wag, co zmniejsza wariancję modelu. Regularyzacja $L^2$ jest standardowo stosowana w większości sieci (często z bardzo małym współczynnikiem $\alpha$, rzędu $10^{-4}$ lub mniejszym). Alternatywą jest **regularyzacja $L^1$**: $\Omega(\Theta) = \alpha|\mathbf{w}|\_1 = \alpha \sum\_i |w\_i|$. Powoduje ona zanikanie wag nie przez ich pomniejszanie, lecz przez **zerowanie wielu z nich** – prowadzi do rozwiązania rzadkiego (wiele wag dokładnie 0). Można to interpretować jako formę automatycznej **selekcji cech** – sieć sama wygasza nieistotne połączenia. Regularyzacja $L^1$ rzadziej jest używana w czystej postaci, ale bywa łączona z $L^2$ (tzw. regu penalizacji *Elastic Net*). Z praktycznych uwag: w sieciach neuronowych zazwyczaj **karze podlegają tylko wagi połączeń, a nie biasy (progi)**. Dzieje się tak, gdyż wagi dotyczą interakcji cech i łatwiej ich nadmiarowość (duże wartości) prowadzi do nadmiernego dopasowania, podczas gdy progi reprezentują indywidualne skalowanie neuronów i ich regularyzacja często niepotrzebnie utrudnia uczenie (może wprowadzać niedouczenie).

* **Złożoność modelu** – pewna forma regularyzacji to też **ograniczanie wielkości modelu** (liczby parametrów). Sieci o mniejszej liczbie warstw czy neuronów mają mniejszą pojemność i trudniej je przeuczyć. Oczywiście zbyt mały model może z kolei nie nauczyć się poprawnie zadania (niedouczenie). Projektując architekturę często stosuje się zasadę **brzytwy Ockhama** – wybieraj model najprostszy, który jest w stanie osiągnąć akceptowalne wyniki. W razie potrzeby zwiększa się jego złożoność stopniowo. W praktyce poszukuje się balansu między niedouczeniem a nadmiernym dopasowaniem.

* **Dropout** – obecnie bardzo popularna metoda regularyzacji zaproponowana w 2014 r. Polega na **losowym “wyłączaniu” pewnej części neuronów podczas uczenia**. W każdej iteracji trenowania dla każdego neuronu (zazwyczaj warstwy ukrytej) losuje się niezależnie zmienną Bernoulliego, która z pewnym prawdopodobieństwem $p$ przyjmuje wartość 0 – wtedy neuronu nie bierze się pod uwagę w obliczeniach (ani jego wyjścia, ani pochodnych) w tej iteracji. To tak jakby za każdym razem trenować nieco inny archipelag sieci. Dropout przeciwdziała sytuacji, gdy neurony nadmiernie dostosowują się do siebie (tzw. ko-adaptacja) – **wymusza bardziej rozproszone, uśrednione reprezentacje**. Efektem jest często znaczne zmniejszenie overfittingu. Po zakończeniu uczenia, przy wykonaniu sieci na nowych danych, wszystkie neurony są obecne, ale ich wyjścia skalujemy o czynnik $p$ (średnie prawdopodobieństwo bycia aktywnym) – to kompensuje sumaryczną siłę sygnałów. Dropout jest prosty i skuteczny, często dodaje się go do dużych modeli (np. w sieciach konwolucyjnych stosuje się dropout na warstwach w pełni połączonych). Zaleca się ostrożność tylko przy bardzo małych zbiorach danych lub zbyt dużym $p$ (typowo $p=0.5$ dla warstw ukrytych).

* **Early stopping (wczesne zatrzymanie)** – metoda polegająca na **przerwaniu treningu zanim model w pełni zminimalizuje błąd na zbiorze treningowym**, w momencie gdy zaczyna pogarszać się na zbiorze walidacyjnym. Realizuje się to poprzez monitorowanie wartości funkcji kosztu (lub miary jakości) na zbiorze walidacyjnym podczas trenowania. Zwykle po każdej *epoce* (przejściu przez cały zbiór treningowy) sprawdzamy błąd walidacyjny; jeżeli widzimy, że przestał maleć i zaczyna rosnąć (co oznacza przeuczenie – model “nauczył się” już szumów zamiast sygnału), przerywamy trening i przyjmujemy najlepszą znalezioną konfigurację wag. Early stopping jest prostym i bardzo skutecznym zabezpieczeniem przed overfittingiem – de facto **wybiera model o mniejszej liczbie iteracji optymalizacji**, co można interpretować jako pewną formę ograniczenia złożoności (mniej “dopasowywania się” do danych).

* **Augmentacja danych (wzbogacanie zbioru)** – choć nie jest to metoda regularyzacji modelu per se, a raczej trick z danymi, warto wspomnieć: jednym z najlepszych sposobów poprawy uogólniania jest zapewnienie **większej ilości danych treningowych**. Jeśli nie możemy zebrać nowych danych, często da się **sztucznie zwiększyć zbiór uczący poprzez generowanie przekształconych przykładów**. Szczególnie w rozpoznawaniu obrazów stosuje się np. losowe przekształcenia geometryczne (obroty, skale, lekkie przesunięcia, zmiany jasności/kontrastu) istniejących obrazów – tak, by dla człowieka obraz nadal przedstawiał to samo, a dla modelu był nowym przykładem. W ten sposób model uczy się odporności na pewne zmiany i nie przeucza się do konkretnych ułożeń pikseli. Augmentacja jest powszechnie stosowana w wizji komputerowej, a analogiczne metody wymyślono dla dźwięku czy tekstu (np. drobne modyfikacje audio, parafrazy zdań itp.). Zwiększenie różnorodności danych czyni problem trudniejszym do “wykucia na pamięć”, więc działa jak regularyzacja.

* **Inne**: Do metod regularyzacji zalicza się też m.in. **normalizację batchową** (ang. *batch normalization* – stabilizuje rozkłady aktywacji w warstwach, co pośrednio daje efekt regularyzujący), **pruning** (odcinanie najmniej istotnych połączeń po trenowaniu, by uprościć model), czy też **ensamble modeli** (traktowane czasem jako regularyzacja poprzez uśrednienie wielu modeli). Jednak dwiema najczęściej stosowanymi w sieciach pozostają *weight decay* i *dropout*, ewentualnie early stopping.

## Śledzenie procesu uczenia i dobór architektury

Trenowanie sieci neuronowej wymaga nie tylko uruchomienia algorytmu, ale też **monitorowania jego przebiegu** i ewentualnego dostrajania założeń modelu. Kilka aspektów, na które zwraca się uwagę:

**Dobór wstępnej architektury:** Na początku, nie znając jeszcze najlepszego rozwiązania, warto zacząć od **prostego modelu referencyjnego** i stopniowo zwiększać jego złożoność. Taki **baseline** daje punkt odniesienia – jeśli nawet bardzo złożony model nie przebija prostego, to znak, że coś jest nie tak. W uczeniu maszynowym często jako bazę dla klasyfikacji wybiera się np. *regresję logistyczną* (model liniowy). Dla bardziej złożonych problemów obrazowych czy językowych – sensownym baseline będzie już głębszy model, ale o znanej, umiarkowanej architekturze (np. sieć konwolucyjna średniej wielkości dla obrazów, lub prosty model LSTM dla sekwencji). **W kontekście doboru architektury do danych** przyjmuje się: jeśli dane to uporządkowane wektory o ustalonej długości – zaczynamy od zwykłej sieci w pełni połączonej; jeśli dane mają lokalną strukturę (np. piksele obrazu w 2D) – używamy sieci konwolucyjnej; jeśli dane tworzą sekwencję o zmiennej długości – stosujemy sieć rekurencyjną (LSTM/GRU). Te domyślne wybory wynikają z właściwości danych i modeli (opisanych wcześniej). Oczywiście są od nich odstępstwa, ale stanowią dobry punkt startu.

**Monitorowanie błędu treningowego i walidacyjnego:** Podczas uczenia śledzimy, jak zmienia się wartość funkcji kosztu (lub inna miara błędu) na zbiorze treningowym i na zbiorze walidacyjnym w kolejnych epokach. Typowo sporządza się **wykresy learning curve** – błąd vs numer epoki. Ich analiza pozwala zdiagnozować:

* **Niedouczenie (underfitting)** – gdy **błąd treningowy jest wysoki** i niemal równy błędowi walidacyjnemu. Model nie zdołał dobrze nauczyć się zależności nawet na treningu. Przyczyną może być zbyt mała złożoność modelu (np. za mało neuronów, za płytka sieć) albo niewłaściwa architektura, ewentualnie problemy optymalizacji (np. zbyt duży lub zbyt mały współczynnik uczenia). Rozwiązaniem jest zwiększenie pojemności modelu (więcej parametrów), dłuższe trenowanie lub zmiana hiperparametrów.
* **Przetrenowanie (overfitting)** – gdy **błąd treningowy jest niski, a błąd walidacyjny wyraźnie wyższy** (zaczyna rosnąć podczas dalszych iteracji). Model nauczył się specyfiki danych uczących aż za dobrze, kosztem zdolności generalizacji. Wykres walidacyjny często ma wtedy minimum w pewnym momencie, po którym błąd rośnie – to sygnał do zastosowania regularyzacji lub zakończenia treningu (patrz *early stopping*). W takiej sytuacji można: zwiększyć regularyzację (silniejsze *weight decay*, wyższy dropout, itp.), użyć wcześniejszej przerwy w trenowaniu, albo – co często najskuteczniejsze – **dostarczyć więcej danych treningowych** (jeśli to możliwe). Czasem też pomaga uproszczenie modelu (mniejsza liczba parametrów), bo wtedy trudniej mu dostroić się do szumu w danych.

Najlepiej, gdy krzywe treningu i walidacji zbiegną do podobnej wartości niskiego błędu – świadczy to o dobrym dopasowaniu bez przeuczenia. W praktyce zawsze istnieje pewna luka między nimi (błąd walidacyjny wyższy niż treningowy), ale chodzi o to, by była jak najmniejsza.

**Dobór hiperparametrów:** Podczas trenowania możemy eksperymentalnie dostrajać takie parametry procesu jak wsp. uczenia, rozmiar minibatcha, współczynniki regularyzacji itp. Stosuje się do tego zbiór walidacyjny – testując różne konfiguracje i wybierając najlepszą. Należy robić to ostrożnie, by nie doprowadzić do *wycieku informacji* ze zbioru walidacyjnego (jeśli zbyt wiele razy sprawdzimy różne ustawienia, w pewnym sensie *dopasujemy się* do walidacji). Dlatego czasem wydziela się jeszcze jeden mini-zbiór lub stosuje cross-validację. Niemniej w deep learningu powszechne jest pewne iteracyjne strojenie na walidacji.

**Ocena na zbiorze testowym:** Po zakończonym trenowaniu (i tuningowaniu), finalną ocenę modelu przeprowadza się na **niezależnym zbiorze testowym**, który nie brał udziału w żadnym etapie uczenia. Tutaj uzyskana miara jakości jest wyznacznikiem spodziewanego działania modelu w realnych warunkach. Np. dla klasyfikacji binarnej często raportuje się accuracy, precyzję, czułość, *F-miary* itp., a dla regresji – błąd średniokwadratowy, MAE, itp. Ważne by używać odpowiednich metryk dostosowanych do problemu (inne dla niezrównoważonych klas, inne dla regresji z outlierami itd.).

Podsumowując, proces uczenia sieci to nie tylko uruchomienie algorytmu optymalizacji, ale **cały cykl eksperymentów**: projekt architektury, trenowanie, monitorowanie błędów, regularyzacja i ewentualnie modyfikacja podejścia. Dzięki uważnej analizie krzywych błędu można wykryć problemy (niedouczenie/przeuczenie) i zawczasu je korygować, dobierając właściwe techniki regularyzacji lub rozmiar modelu. Nowoczesne biblioteki (TensorBoard itp.) bardzo ułatwiają śledzenie tych metryk podczas uczenia.

**Dobór modelu do problemu:** Na koniec warto zaznaczyć, że wybór typu sieci i jej wielkości zawsze powinien być uzasadniony naturą danych i problemu. Jak wspomniano, obrazy “proszą się” o sieci konwolucyjne, sekwencje o rekurencyjne, a dane tabelaryczne – o klasyczne gęste MLP (ew. modele oparte o drzewa decyzyjne, które czasem lepiej się sprawdzają na tego typu danych). Złożoność architektury należy zwiększać tylko w razie potrzeby. Często zaleca się zasadę: *“najmniejszy model, który jest w stanie się nauczyć”*. Dopiero jeśli mały model ma wyraźny niedobór mocy (niedoucza się), zwiększamy liczbę parametrów. Takie podejście zapobiega niepotrzebnemu przeuczeniu i zmniejsza koszty obliczeniowe. W problemach praktycznych istotny jest też balans między dokładnością a złożonością – czasem model minimalnie gorszy, ale dużo mniejszy, jest preferowany (np. do implementacji w urządzeniu mobilnym).

**Bibliografia:**

* Marcin Sydow, *Wprowadzenie do uczenia maszynowego* – materiały koncepcyjno-teoretyczne (cytaty w tekście).